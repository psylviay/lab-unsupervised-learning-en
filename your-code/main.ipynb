{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the wholesale customer data\n",
    "df = pd.read_csv('../data/Wholesale customers data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explores the wholesale customer data file\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loads the dataset\n",
    "data = pd.read_csv('../data/Wholesale customers data.csv')\n",
    "\n",
    "# Provides general information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for missing data\n",
    "print(\"Missing Data Summary:\")\n",
    "missing_data = data.isnull().sum()\n",
    "print(missing_data)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays descriptive statistics for the data\n",
    "print(\"Descriptive Statistics:\")\n",
    "descriptive_stats = data.describe()\n",
    "print(descriptive_stats)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes and displays a correlation analysis for the data\n",
    "print(\"Correlation Matrix:\")\n",
    "correlation_matrix = data.corr()\n",
    "print(correlation_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generates a correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays data distribution and outliers in a visual manner\n",
    "\n",
    "for col in data.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducts skewness and Kurtosis analysis\n",
    "print(\"Skewness and Kurtosis Analysis:\")\n",
    "for col in data.columns:\n",
    "    skewness = data[col].skew()\n",
    "    kurtosis = data[col].kurt()\n",
    "    print(f\"{col}: Skewness = {skewness:.2f}, Kurtosis = {kurtosis:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducts the pareto analysis and displays a pareto plot\n",
    "\n",
    "# Conducts the pareto analysis\n",
    "data['Total_Spend'] = data[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']].sum(axis=1)\n",
    "data = data.sort_values(by='Total_Spend', ascending=False)\n",
    "data['Cumulative_Spend'] = data['Total_Spend'].cumsum() / data['Total_Spend'].sum()\n",
    "\n",
    "# Generates the pareto plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['Cumulative_Spend'].values, label=\"Cumulative Spend\", color=\"blue\")\n",
    "plt.axhline(y=0.8, color=\"red\", linestyle=\"--\", label=\"80% Threshold\")\n",
    "plt.title(\"Pareto Analysis: Cumulative Spend\")\n",
    "plt.xlabel(\"Customers (sorted by spending)\")\n",
    "plt.ylabel(\"Cumulative Spend Ratio\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- **General Overview**\n",
    "- Dataset consists of 8 columns and 440 entries.\n",
    "- All columns are numeric.\n",
    "- Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen show quantitative spending for each corresponding category.\n",
    "- Channel and Region are categorical variables represented as integers.\n",
    "\n",
    "- **Missing Values**\n",
    "- No missing data was found within the data.\n",
    "\n",
    "- **Skewness and Kurtosis**\n",
    "- The histograms for Channel and Region reflect the distribution of the entries based on their corresponding discrete categories.\n",
    "- The histograms for the other columns show strong positive skewness\n",
    "- Delicassen, and Frozen show significant skewness and kurtosis (Milk and Grocery as well, but not as strongly)\n",
    "\n",
    "- **Correlation Analysis**\n",
    "- Grocery and Detergents_Paper have a strong correlation (0.92).\n",
    "- It appears that Milk, Grocery and Detergents_Paper are highly correlated with each other.\n",
    "- These high correlations should be looked at since they could result in multicollinearity in the models.\n",
    "\n",
    "- **Outliers**\n",
    "- There appears to be extreme outliers in several of the columns. These should be investigated to determine whether or not they should be removed.\n",
    "\n",
    "- **Pareto Analysis**\n",
    "- The data does reflect that ~20% of the customers provide 80% of the total spending.\n",
    "\n",
    "- Net, Channel and Region should be transformed into categorical variables.\n",
    "- Skewness and outliers need to be addressed.\n",
    "- Multicollinearity needs to be investigated and addressed.\n",
    "- Also, from the Pareto analysis, customers with the highest spending should be looked at.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts Channel and Region to categorical variables\n",
    "data['Channel'] = data['Channel'].astype('category')\n",
    "data['Region'] = data['Region'].astype('category')\n",
    "\n",
    "# Checks if the change was made correctly\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses log transformation to address high skewness in some of the columns\n",
    "# log1p(x) = log(1 + x), handles 0 values\n",
    "columns_to_transform = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "for col in columns_to_transform:\n",
    "    data[col] = np.log1p(data[col])  \n",
    "\n",
    "# Displays data distribution and outliers in a visual manner\n",
    "for col in data.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses PCA to address multicollinearity\n",
    "\n",
    "# Standardizes the colums for PCA (numerical columns)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data[columns_to_transform])\n",
    "\n",
    "# Applies PCA\n",
    "pca = PCA(n_components=2)  \n",
    "pca_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Adds PCA components to the dataframe\n",
    "data['PCA1'] = pca_components[:, 0]\n",
    "data['PCA2'] = pca_components[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses KMeans clustering to segment customers based on their spending\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Applies KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # 3 clusters\n",
    "data['Customer_Segment'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "# Uses PCA components to look at the segments\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='Customer_Segment', data=data, palette='viridis', s=50)\n",
    "plt.title(\"Customer Segments Based on PCA Components\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title=\"Segment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculates the pareto analysis\n",
    "data['Total_Spend'] = data[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']].sum(axis=1)\n",
    "data = data.sort_values(by='Total_Spend', ascending=False)\n",
    "data['Cumulative_Spend'] = data['Total_Spend'].cumsum() / data['Total_Spend'].sum()\n",
    "\n",
    "# Generates the new pareto plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['Cumulative_Spend'].values, label=\"Cumulative Spend\", color=\"blue\")\n",
    "plt.axhline(y=0.8, color=\"red\", linestyle=\"--\", label=\"80% Threshold\")\n",
    "plt.title(\"Pareto Analysis: Cumulative Spend (After Transformation)\")\n",
    "plt.xlabel(\"Customers (sorted by spending)\")\n",
    "plt.ylabel(\"Cumulative Spend Ratio\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the transformed data\n",
    "processed_data_path = '../data/Processed_Wholesale_Customers_Data.csv'\n",
    "data.to_csv(processed_data_path, index=False)\n",
    "print(f\"Processed data saved to {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments after the data transformation**\n",
    "\n",
    "- More symmetry can now be observed across the data distributions.\n",
    "- Through PCA  the spend columns were transformed into 2 principal components that explain most of the variance.\n",
    "- Three customer segments can now be identified so marketing, advertising and\n",
    "- loyalty campaigns can be developed for them, particularly the high spenders, who represent ~80% of the spend.\n",
    "- Their particular spending characteristics can be evaluated to obtain further insights that can be used in strategy development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Defines the columns to scale (exclude non-numeric or already transformed columns if necessary)\n",
    "columns_to_scale = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "\n",
    "# Initializes the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fits the scaler on the selected columns and transforms the data\n",
    "customers_scale = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "# Converts the scaled data back into a DataFrame for easier interpretation (optional)\n",
    "customers_scale_df = pd.DataFrame(customers_scale, columns=columns_to_scale)\n",
    "\n",
    "# Displays the scaled data\n",
    "print(customers_scale_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initializes the KMeans model with a 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fits the KMeans model on the scaled data\n",
    "kmeans.fit(customers_scale)\n",
    "\n",
    "# Gets the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Assigns the labels back to the original data in a new column\n",
    "data['labels'] = labels\n",
    "\n",
    "# Displays the data with cluster labels\n",
    "print(data[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen', 'labels']].head())\n",
    "\n",
    "# Visualizes the cluster distribution (optional)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displays the cluster distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='labels', data=data, hue='labels', palette='viridis', dodge=False, legend=False)\n",
    "plt.title(\"Cluster Distribution\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the plot above, we can choose to have 2 clusters.\n",
    "# This re-runs the analysis based on 2 clusters, and assigns the cluster labels back to the original data.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Performs the KMeans clustering with 2 clusters\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42).fit(customers_scale)\n",
    "\n",
    "# Predicts the clusters for the scaled data\n",
    "labels = kmeans_2.predict(customers_scale)  # Alternatively, use kmeans_2.labels_\n",
    "\n",
    "# Converts the cluster labels to a list\n",
    "clusters = labels.tolist()\n",
    "\n",
    "# Assigns the cluster labels back to the original data\n",
    "data['Cluster_2'] = clusters\n",
    "\n",
    "# Displays the data with cluster labels\n",
    "print(data[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen', 'Cluster_2']].head())\n",
    "\n",
    "# Visualizes the cluster distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Cluster_2', data=data, palette='viridis')\n",
    "plt.title(\"Cluster Distribution (2 Clusters)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the values in labels using Counter.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from collections import Counter\n",
    "\n",
    "# Counts the occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Displays the counts\n",
    "print(\"Cluster Counts:\")\n",
    "for cluster, count in label_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clusters the data using DBSCAN.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Initializes the DBSCAN model with epsilon set to 0.5\n",
    "dbscan = DBSCAN(eps=0.5)\n",
    "\n",
    "# Fits the DBSCAN model to the scaled data\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# Gets the cluster labels from DBSCAN\n",
    "dbscan_labels = dbscan.labels_\n",
    "\n",
    "# Adds the DBSCAN labels to the original dataset\n",
    "data['labels_DBSCAN'] = dbscan_labels\n",
    "\n",
    "# Displays the data with both K-Means and DBSCAN labels\n",
    "print(data[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen', 'Cluster_2', 'labels_DBSCAN']].head())\n",
    "\n",
    "# Visualizes the DBSCAN cluster distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='labels_DBSCAN', hue='labels_DBSCAN', data=data, palette='viridis', legend=False)\n",
    "plt.title(\"Cluster Distribution (DBSCAN)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences in the DBSCAN labels column\n",
    "dbscan_label_counts = data['labels_DBSCAN'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"DBSCAN Cluster Counts:\")\n",
    "print(dbscan_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the column pairs for the scatter plots\n",
    "column_pairs = [\n",
    "    ('Detergents_Paper', 'Milk'),\n",
    "    ('Grocery', 'Fresh'),\n",
    "    ('Frozen', 'Delicassen')\n",
    "]\n",
    "\n",
    "# Creates scatter plots for each column pair\n",
    "for x_col, y_col in column_pairs:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Generates scatter plots for K-Means labels\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(\n",
    "        x=x_col, y=y_col, hue='Cluster_2', data=data, palette='viridis', s=50\n",
    "    )\n",
    "    plt.title(f\"K-Means Clustering: {x_col} vs {y_col}\")\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "\n",
    "    # Generates the scatter plot for DBSCAN labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(\n",
    "        x=x_col, y=y_col, hue='labels_DBSCAN', data=data, palette='viridis', s=50\n",
    "    )\n",
    "    plt.title(f\"DBSCAN Clustering: {x_col} vs {y_col}\")\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "\n",
    "    # Shows the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excludes the categorical columns 'Region' and 'Channel'\n",
    "columns_to_analyze = data.drop(columns=['Region', 'Channel'])\n",
    "\n",
    "# Groups by K-Means labels and calculates the mean for all numeric columns\n",
    "kmeans_means = columns_to_analyze.groupby(data['Cluster_2']).mean()\n",
    "\n",
    "print(\"Mean Values by K-Means Clusters:\")\n",
    "print(kmeans_means)\n",
    "\n",
    "# Groups by DBSCAN labels and calculates the mean for all numeric columns\n",
    "dbscan_means = columns_to_analyze.groupby(data['labels_DBSCAN']).mean()\n",
    "\n",
    "print(\"\\nMean Values by DBSCAN Clusters:\")\n",
    "print(dbscan_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- KMeans clusters are easier to identify because they appear to be more distinct. For example,\n",
    "- Cluster 0 customers are higher spenders on Milk, Grocery and Detergents_Paper, whereas cluster 1\n",
    "- customers are higher spenders on Fresh and Frozen.\n",
    "\n",
    "- DBSCAN identified noise points and separated these from cluster 0. These noise points could be outliers or lower spenders.\n",
    "- Also, most customers can be found within cluster 0, making the segmentation less distinct. This leads me to believe that\n",
    "- even though noise points are identified by DBSCAN, spending patters are not as straightforward to identify as with K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates scatterplots with various numbers of clusters (1-5).\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Defines the number of clusters to test\n",
    "cluster_range = range(1, 6)\n",
    "\n",
    "# Defines a column pair to visualize (e.g., 'Detergents_Paper' vs 'Milk')\n",
    "x_col, y_col = 'Detergents_Paper', 'Milk'\n",
    "\n",
    "# Iterates through different numbers of clusters\n",
    "for n_clusters in cluster_range:\n",
    "    # Fit K-Means with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    data[f'Cluster_{n_clusters}'] = kmeans.fit_predict(customers_scale)\n",
    "\n",
    "    # Creates a scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        x=x_col, y=y_col, hue=f'Cluster_{n_clusters}', data=data, palette='viridis', s=50\n",
    "    )\n",
    "    plt.title(f\"K-Means Clustering with {n_clusters} Clusters: {x_col} vs {y_col}\")\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "- Visually speaking, the solution with 2 clusters appears to be the better option among the scatter plots\n",
    "- above. There are 2 distinct segments with very little overlap. From 3 clusters onward, there is much overlap\n",
    "- between the clusters, making them harder to distinguish.\n",
    "\n",
    "-I have included code below to experiment with WCSS and silhouette scores and see what the optimal number of \n",
    "-clusters would be for each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the WCSS for various numbers of clusters and plots the elbow curve.\n",
    "\n",
    "# Calculates the WCSS for different numbers of clusters\n",
    "wcss = []\n",
    "for n_clusters in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(customers_scale)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plots the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, it appears that between 2 and 3 is where the sharpest drop occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and plots silhouette scores for various number of clusters.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculates the silhouette scores for different numbers of clusters\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in range(2, 11):  \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(customers_scale)\n",
    "    score = silhouette_score(customers_scale, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plots the Silhouette Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Analysis for Optimal Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model with 2 clusters has, the highest silhouette score. This means that it has better defined clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments changing the eps and min_samples parameters for DBSCAN. Scatter plots are generated\n",
    "# to visualize the results.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Defines the parameters to experiment with\n",
    "eps_values = [0.3, 0.5, 0.7]\n",
    "min_samples_values = [3, 5, 10]\n",
    "\n",
    "# Defines the features for scatter plot visualization\n",
    "x_col, y_col = 'Detergents_Paper', 'Milk'\n",
    "\n",
    "# Iterates over combinations of eps and min_samples\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        # Initialize and fit DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(customers_scale)\n",
    "\n",
    "        # Adds DBSCAN labels to a temporary DataFrame for visualization\n",
    "        temp_data = data.copy()\n",
    "        temp_data['DBSCAN_labels'] = labels\n",
    "\n",
    "        # Creates the scatter plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            x=x_col, y=y_col, hue='DBSCAN_labels', data=temp_data, palette='viridis', s=50\n",
    "        )\n",
    "        plt.title(f\"DBSCAN Clustering: eps={eps}, min_samples={min_samples}\")\n",
    "        plt.xlabel(x_col)\n",
    "        plt.ylabel(y_col)\n",
    "        plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- As with modeling in general, iteration/experimentation is ideal to determining the ideal eps/sample combination.\n",
    "- Lower min_samples values results in fewer noise points, but also more clusters that may be less meaningful may emerge.\n",
    "- With higher min_samples fewer clusters are formed, but with more noise points.\n",
    "\n",
    "-Depending on the task at hand, having looser vs more compact clusters may be ideal. The same goes for the acceptable\n",
    "level of noise. The idea should be to keep the combination that minimizes noise but reflects more distinct, and meaningful,\n",
    "clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
